experiment_name: gsm8k-grpo
trial_name: trial0
allocation_mode: vllm.d4p1t2+d8p1t1
weight_update_mode: xccl
cluster:
  fileroot: /tmp/lite
  n_nodes: 2
  n_gpus_per_node: 8
  name_resolve:
    type: nfs
    nfs_record_root: /tmp/lite/name_resolve
seed: 1
total_train_epochs: 10
tokenizer_path: ${actor.path}
async_training: true

rollout:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  max_concurrent_rollouts: 16
  queue_size: null
  consumer_batch_size: ${train_dataset.batch_size}
  max_head_offpolicyness: 2
  enable_rollout_tracing: false

gconfig:
  n_samples: 4
  min_new_tokens: 0
  max_new_tokens: 12288
  greedy: false
  temperature: 1.0
  interruptable_processpool: true

actor:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  path: Qwen/Qwen2.5-1.5B-Instruct
  init_from_scratch: false
  disable_dropout: true
  gradient_checkpointing: true
  dtype: bfloat16
  mb_spec:
    max_tokens_per_mb: 20480
  optimizer:
    type: adam
    lr: 1.70e-5
    weight_decay: 0.017
    beta1: 0.9
    beta2: 0.999
    eps: 1e-8
    lr_scheduler_type: constant
    gradient_clipping: 1.0
    warmup_steps_proportion: 0.001
  backend: fsdp
  profiler:
    enable: false
    save_path: "./profiler_dir"
    level: "level1"
    step_start: 0
    step_end: 1
    profile_ranks: [0]
  prof_mem:
    enable: false
    save_dir: ./prof_memory_log

  group_size: ${gconfig.n_samples}
  group_adv_norm: false
  eps_clip: 0.4
  temperature: ${gconfig.temperature}
  reward_scaling: 10.0
  reward_bias: -0.5
  kl_ctl: 0.0
  ppo_n_minibatches: 1
  recompute_logprob: true
  use_decoupled_loss: true
  behav_imp_weight_cap: 5.0

ref:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  path: ${actor.path}
  init_from_scratch: false
  disable_dropout: true
  dtype: ${actor.dtype}
  mb_spec:
    max_tokens_per_mb: 10240
  optimizer: null
  backend: fsdp

vllm:
  model: ${actor.path}
  seed: ${seed}
  skip_tokenizer_init: false
  dtype: ${actor.dtype}
  max_model_len: 32768
  gpu_memory_utilization: 0.9

# datasets
train_dataset:
  batch_size: 8
  shuffle: true
  pin_memory: true
  num_workers: 4
  distributed_load: false
  path: openai/gsm8k
  type: rl

valid_dataset:
  batch_size: 8
  shuffle: true
  pin_memory: true
  num_workers: 4
  path: openai/gsm8k
  type: rl

# Utilities
saver:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  fileroot: ${cluster.fileroot}
  freq_epochs: 1
  freq_steps: null
  freq_secs: null

recover:
  mode: disabled
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  fileroot: ${cluster.fileroot}
  freq_epochs: 1
  freq_steps: null
  freq_secs: 3600

evaluator:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  fileroot: ${cluster.fileroot}
  freq_epochs: 1
  freq_steps: null
  freq_secs: null

stats_logger:
  experiment_name: ${experiment_name}
  trial_name: ${trial_name}
  fileroot: ${cluster.fileroot}
  wandb:
    mode: disabled

launcher:
  inference_server_cpus_per_gpu: 4
  inference_server_mem_per_gpu: 32768
  trainer_cpus_per_gpu: 4
  trainer_mem_per_gpu: 32768
  inference_server_env_vars: 'VLLM_USE_V1=1,TASK_QUEUE_ENABLE=2,VLLM_VERSION=0.9.1,ASCEND_PROCESS_LOG_PATH=/home/admin/logs/ascend/log,HCCL_EXEC_TIMEOUT=7200'
  trainer_env_vars: 'ASCEND_PROCESS_LOG_PATH=/home/admin/logs/ascend/log,HCCL_EXEC_TIMEOUT=7200,PYTORCH_NPU_ALLOC_CONF=expandable_segments:True'
